package main

import (
	"context"
	"fmt"
	"net/http"
	"time"

	"waverless/app/handler"
	"waverless/app/router"
	"waverless/internal/model"
	"waverless/internal/service"
	endpointsvc "waverless/internal/service/endpoint"
	"waverless/pkg/autoscaler"
	"waverless/pkg/capacity"
	"waverless/pkg/config"
	"waverless/pkg/deploy/k8s"
	"waverless/pkg/deploy/novita"
	"waverless/pkg/interfaces"
	"waverless/pkg/logger"
	"waverless/pkg/monitoring"
	"waverless/pkg/provider"
	mysqlstore "waverless/pkg/store/mysql"
	redisstore "waverless/pkg/store/redis"

	awsconfig "github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/credentials"
	"github.com/aws/aws-sdk-go-v2/service/ec2"
	appsv1 "k8s.io/api/apps/v1"

	"github.com/gin-gonic/gin"
)

// initConfig initializes configuration
func (app *Application) initConfig() error {
	if err := config.Init(); err != nil {
		return err
	}
	app.config = config.GlobalConfig
	return nil
}

// initLogger initializes logging
func (app *Application) initLogger() error {
	if err := logger.Init(); err != nil {
		return err
	}
	app.registerCleanup(func() {
		logger.Sync()
		logger.InfoCtx(app.ctx, "Logging system has been closed")
	})
	return nil
}

// initMySQL initializes MySQL
func (app *Application) initMySQL() error {
	dsn := fmt.Sprintf("%s:%s@tcp(%s:%d)/%s?charset=utf8mb4&parseTime=True&loc=UTC",
		app.config.MySQL.User,
		app.config.MySQL.Password,
		app.config.MySQL.Host,
		app.config.MySQL.Port,
		app.config.MySQL.Database,
	)

	repo, err := mysqlstore.NewRepository(dsn, app.config.MySQL.Proxy)
	if err != nil {
		return err
	}

	app.mysqlRepo = repo
	app.registerCleanup(func() {
		repo.Close()
		logger.InfoCtx(app.ctx, "MySQL connection has been closed")
	})

	return nil
}

// initRedis initializes Redis
func (app *Application) initRedis() error {
	client, err := redisstore.NewRedisClient(app.config)
	if err != nil {
		return err
	}

	app.redisClient = client
	app.registerCleanup(func() {
		client.Close()
		logger.InfoCtx(app.ctx, "Redis connection has been closed")
	})

	return nil
}

// initProviders initializes business providers
func (app *Application) initProviders() error {
	// Initialize Provider Factory
	factory := provider.NewProviderFactory(app.config)

	// Create business providers (Deployment, Queue)
	providers, err := factory.CreateBusinessProviders()
	if err != nil {
		return fmt.Errorf("failed to create business providers: %w", err)
	}

	app.deploymentProvider = providers.Deployment

	// Register cleanup for K8s provider
	if k8sProv, ok := providers.Deployment.(*k8s.K8sDeploymentProvider); ok {
		app.registerCleanup(func() {
			k8sProv.Close()
			logger.InfoCtx(app.ctx, "K8s deployment provider has been closed")
		})
	}

	return nil
}

// initServices initializes service layer
func (app *Application) initServices() error {

	// Initialize worker service (MySQL-based)
	app.workerService = service.NewWorkerService(
		app.mysqlRepo.Worker,
		app.mysqlRepo.Task,
		app.deploymentProvider,
	)

	// Initialize worker event service for monitoring
	app.workerEventService = service.NewWorkerEventService(app.mysqlRepo.Monitoring)
	app.workerService.SetWorkerEventService(app.workerEventService)

	// Initialize endpoint service
	app.endpointService = endpointsvc.NewService(
		app.mysqlRepo.Endpoint,
		app.mysqlRepo.AutoscalerConfig,
		app.mysqlRepo.Task,
		app.workerService,
		app.deploymentProvider,
	)

	// Initialize task service
	app.taskService = service.NewTaskService(
		app.mysqlRepo.Task,
		app.mysqlRepo.TaskEvent,
		app.endpointService,
		app.deploymentProvider,
	)

	// Set task service on worker service (for event recording)
	app.workerService.SetTaskService(app.taskService)

	// Set worker service on task service (for worker stats recording)
	app.taskService.SetWorkerService(app.workerService)

	// Initialize statistics service
	app.statisticsService = service.NewStatisticsService(app.mysqlRepo.TaskStatistics, app.mysqlRepo.Worker)

	// Set statistics service on task service (for incremental statistics updates)
	app.taskService.SetStatisticsService(app.statisticsService)

	// Initialize spec service
	app.specService = service.NewSpecService(app.mysqlRepo.Spec)

	// Initialize monitoring service
	app.monitoringService = service.NewMonitoringService(app.mysqlRepo.Monitoring)

	// Initialize monitoring collector
	app.monitoringCollector = monitoring.NewCollector(app.mysqlRepo.Monitoring, app.mysqlRepo.Worker, app.mysqlRepo.Task)

	// Get K8s deployment provider for draining check
	var k8sDeployProvider *k8s.K8sDeploymentProvider
	if app.config.K8s.Enabled {
		if k8sProv, ok := app.deploymentProvider.(*k8s.K8sDeploymentProvider); ok {
			k8sDeployProvider = k8sProv
		}
	}

	// Get Novita deployment provider for status sync
	var novitaDeployProvider *novita.NovitaDeploymentProvider
	if app.config.Novita.Enabled {
		if novitaProv, ok := app.deploymentProvider.(*novita.NovitaDeploymentProvider); ok {
			novitaDeployProvider = novitaProv
			// Inject spec service for database access
			if app.specService != nil {
				novitaDeployProvider.SetSpecRepository(app.specService)
				logger.InfoCtx(app.ctx, "Spec service injected into Novita provider - specs will be read from database first")
			}
		}
	}

	// Setup Pod watcher for graceful shutdown (when K8s is enabled)
	if err := app.setupPodWatcher(k8sDeployProvider); err != nil {
		logger.WarnCtx(app.ctx, "Failed to setup pod watcher: %v (non-critical, continuing)", err)
		// Non-critical feature, continue startup
	}

	// Setup Spot interruption watcher (when K8s is enabled)
	if err := app.setupSpotInterruptionWatcher(k8sDeployProvider); err != nil {
		logger.WarnCtx(app.ctx, "Failed to setup spot interruption watcher: %v (non-critical, continuing)", err)
		// Non-critical feature, continue startup
	}

	// Setup Deployment watcher for optimized rolling updates (when K8s is enabled)
	if err := app.setupDeploymentWatcher(k8sDeployProvider); err != nil {
		logger.WarnCtx(app.ctx, "Failed to setup deployment watcher: %v (non-critical, continuing)", err)
		// Non-critical feature, continue startup
	}

	// Setup Pod status watcher for worker runtime state sync (when K8s is enabled)
	if err := app.setupPodStatusWatcher(k8sDeployProvider); err != nil {
		logger.WarnCtx(app.ctx, "Failed to setup pod status watcher: %v (non-critical, continuing)", err)
	}

	// Start pod cleanup job for stuck terminating pods (when K8s is enabled)
	if err := app.startPodCleanupJob(k8sDeployProvider); err != nil {
		logger.WarnCtx(app.ctx, "Failed to start pod cleanup job: %v (non-critical, continuing)", err)
		// Non-critical feature, continue startup
	}

	// Setup capacity manager (when K8s is enabled)
	if err := app.setupCapacityManager(k8sDeployProvider); err != nil {
		logger.WarnCtx(app.ctx, "Failed to setup capacity manager: %v (non-critical, continuing)", err)
	}

	// Setup Novita status watcher for endpoint status sync (when Novita is enabled)
	if err := app.setupNovitaStatusWatcher(novitaDeployProvider); err != nil {
		logger.WarnCtx(app.ctx, "Failed to setup Novita status watcher: %v (non-critical, continuing)", err)
		// Non-critical feature, continue startup
	}

	return nil
}

// setupPodWatcher sets up Pod deletion listener for graceful shutdown
// When a Pod is marked for deletion by K8s, automatically mark the corresponding Worker as draining and stop accepting new tasks
func (app *Application) setupPodWatcher(k8sProvider *k8s.K8sDeploymentProvider) error {
	if k8sProvider == nil {
		logger.InfoCtx(app.ctx, "K8s provider not available, skipping pod watcher setup")
		return nil
	}

	logger.InfoCtx(app.ctx, "Setting up pod watcher for graceful shutdown...")

	// Register Pod terminating callback
	err := k8sProvider.WatchPodTerminating(app.ctx, func(podName, endpoint string) {
		logger.InfoCtx(app.ctx, "üîî Pod %s (endpoint: %s) marked for deletion, draining worker...",
			podName, endpoint)

		// 1. Find Worker by PodName
		worker, err := app.workerService.GetWorkerByPodName(app.ctx, endpoint, podName)
		if err != nil {
			logger.WarnCtx(app.ctx, "Pod %s terminating but worker not found: %v", podName, err)
			// Even if worker not found, still mark Pod as draining for observability
			if markErr := k8sProvider.MarkPodDraining(app.ctx, podName); markErr != nil {
				logger.WarnCtx(app.ctx, "Failed to mark pod %s as draining: %v", podName, markErr)
			}
			return
		}

		// 2. Mark Worker as DRAINING (business logic layer: prevent worker from pulling new tasks)
		err = app.workerService.UpdateWorkerStatus(app.ctx, worker.ID, model.WorkerStatusDraining)
		if err != nil {
			logger.ErrorCtx(app.ctx, "Failed to mark worker %s as draining: %v", worker.ID, err)
			return
		}

		logger.InfoCtx(app.ctx, "‚úÖ Worker %s (Pod: %s) marked as DRAINING, will not receive new tasks",
			worker.ID, podName)

		// 3. Mark Pod as DRAINING in K8s (observability: convenient for kubectl to view pod status)
		// Note: No need to set deletion cost here as Pod is already marked for deletion by K8s
		// deletion cost is only used during proactive scale-down to tell K8s which Pod to delete first
		if err := k8sProvider.MarkPodDraining(app.ctx, podName); err != nil {
			logger.WarnCtx(app.ctx, "Failed to mark pod %s as draining in K8s: %v", podName, err)
		} else {
			logger.InfoCtx(app.ctx, "‚úÖ Pod %s marked as draining in K8s (status=draining label)", podName)
		}
	})

	if err != nil {
		return fmt.Errorf("failed to setup pod watcher: %w", err)
	}

	logger.InfoCtx(app.ctx, "‚úÖ Pod watcher registered successfully")
	return nil
}

// setupSpotInterruptionWatcher sets up Spot interruption detection
func (app *Application) setupSpotInterruptionWatcher(k8sProvider *k8s.K8sDeploymentProvider) error {
	if k8sProvider == nil {
		logger.InfoCtx(app.ctx, "K8s provider not available, skipping spot interruption watcher setup")
		return nil
	}

	logger.InfoCtx(app.ctx, "Setting up spot interruption watcher...")

	err := k8sProvider.WatchSpotInterruption(app.ctx, func(podName, endpoint, reason string) {
		logger.WarnCtx(app.ctx, "üö® SPOT INTERRUPTION detected for Pod %s (endpoint: %s), reason: %s",
			podName, endpoint, reason)

		// Find Worker by PodName
		worker, err := app.workerService.GetWorkerByPodName(app.ctx, endpoint, podName)
		if err != nil {
			logger.WarnCtx(app.ctx, "Spot interruption detected but worker not found for pod %s: %v", podName, err)
			return
		}

		// Immediately mark Worker as DRAINING (2-minute grace period starts now)
		err = app.workerService.UpdateWorkerStatus(app.ctx, worker.ID, model.WorkerStatusDraining)
		if err != nil {
			logger.ErrorCtx(app.ctx, "Failed to mark worker %s as draining after spot interruption: %v", worker.ID, err)
			return
		}

		logger.InfoCtx(app.ctx, "‚úÖ Worker %s (Pod: %s) marked as DRAINING due to spot interruption - stopping new task acceptance",
			worker.ID, podName)

		// Log current jobs for monitoring
		if len(worker.JobsInProgress) > 0 {
			logger.InfoCtx(app.ctx, "üìã Worker %s has %d jobs in progress that will continue: %v",
				worker.ID, len(worker.JobsInProgress), worker.JobsInProgress)
		} else {
			logger.InfoCtx(app.ctx, "üìã Worker %s has no jobs in progress - can terminate safely", worker.ID)
		}

		// Mark Pod as draining for observability
		if err := k8sProvider.MarkPodDraining(app.ctx, podName); err != nil {
			logger.WarnCtx(app.ctx, "Failed to mark pod %s as draining: %v", podName, err)
		}
	})

	if err != nil {
		return fmt.Errorf("failed to setup spot interruption watcher: %w", err)
	}

	logger.InfoCtx(app.ctx, "‚úÖ Spot interruption watcher setup complete")
	return nil
}

// setupNovitaStatusWatcher sets up Novita status watcher for endpoint status sync
func (app *Application) setupNovitaStatusWatcher(novitaProvider *novita.NovitaDeploymentProvider) error {
	if novitaProvider == nil {
		logger.InfoCtx(app.ctx, "Novita provider not available, skipping status watcher setup")
		return nil
	}

	logger.InfoCtx(app.ctx, "Setting up Novita status watcher for endpoint status sync...")

	// Register replica watch callback to sync status to database
	err := novitaProvider.WatchReplicas(app.ctx, func(event interfaces.ReplicaEvent) {
		endpoint := event.Name

		// Calculate status based on replica state
		status := "Pending"
		if event.AvailableReplicas == event.DesiredReplicas && event.DesiredReplicas > 0 {
			status = "Running"
		} else if event.DesiredReplicas == 0 {
			status = "Stopped"
		}

		// Update endpoint runtime state in database
		if app.mysqlRepo != nil && app.mysqlRepo.Endpoint != nil {
			runtimeState := map[string]interface{}{
				"replicas":          event.DesiredReplicas,
				"readyReplicas":     event.ReadyReplicas,
				"availableReplicas": event.AvailableReplicas,
			}

			if err := app.mysqlRepo.Endpoint.UpdateRuntimeState(app.ctx, endpoint, status, runtimeState); err != nil {
				logger.ErrorCtx(app.ctx, "Failed to update Novita endpoint runtime state: %v", err)
			}
		}
	})

	if err != nil {
		logger.WarnCtx(app.ctx, "Failed to register Novita status watcher: %v", err)
		return err
	}

	return nil
}

// setupDeploymentWatcher sets up Deployment change listener (optimizes rolling updates)
// This watcher only sets Pod Deletion Cost to guide K8s on which pods to delete first
// It does NOT mark workers as DRAINING - that's handled by setupPodWatcher when pods are actually terminated
func (app *Application) setupDeploymentWatcher(k8sProvider *k8s.K8sDeploymentProvider) error {
	if k8sProvider == nil {
		logger.InfoCtx(app.ctx, "K8s provider not available, skipping deployment watcher setup")
		return nil
	}

	logger.InfoCtx(app.ctx, "Setting up deployment watcher for optimized rolling updates...")

	// Register Deployment spec change callback
	err := k8sProvider.WatchDeploymentSpecChange(app.ctx, func(endpoint string) {
		logger.InfoCtx(app.ctx, "üîÑ Deployment spec changed for endpoint %s, setting pod deletion priorities...", endpoint)

		// Get all workers for this endpoint
		workers, err := app.workerService.ListWorkers(app.ctx, endpoint)
		if err != nil {
			logger.ErrorCtx(app.ctx, "Failed to get workers for endpoint %s: %v", endpoint, err)
			return
		}

		if len(workers) == 0 {
			logger.InfoCtx(app.ctx, "No workers found for endpoint %s, nothing to optimize", endpoint)
			return
		}

		logger.InfoCtx(app.ctx, "Found %d workers for endpoint %s, setting deletion priorities based on workload...",
			len(workers), endpoint)

		// Set Pod Deletion Cost based on workload
		// This guides K8s to delete idle pods first, busy pods last
		// Workers will be marked as DRAINING by setupPodWatcher when K8s actually deletes them
		idleCount := 0
		busyCount := 0

		for _, worker := range workers {
			podName := worker.ID // worker.ID == podName (from RUNPOD_POD_ID)

			if worker.CurrentJobs == 0 {
				// Idle worker: Set deletion cost to -1000 (highest priority for deletion)
				// K8s will prefer to delete these pods first during rolling update
				if err := k8sProvider.SetPodDeletionCost(app.ctx, podName, -1000); err != nil {
					logger.WarnCtx(app.ctx, "Failed to set deletion cost for idle worker %s: %v", podName, err)
				} else {
					logger.InfoCtx(app.ctx, "‚úÖ Idle worker %s: deletion-cost = -1000 (will be deleted first by K8s)", podName)
					idleCount++
				}
			} else {
				// Busy worker: Set deletion cost to 1000 (lowest priority for deletion)
				// K8s will delete these pods last, giving them time to finish tasks
				if err := k8sProvider.SetPodDeletionCost(app.ctx, podName, 1000); err != nil {
					logger.WarnCtx(app.ctx, "Failed to set deletion cost for busy worker %s: %v", podName, err)
				} else {
					logger.InfoCtx(app.ctx, "‚è≥ Busy worker %s (jobs=%d): deletion-cost = 1000 (will be deleted last by K8s)",
						podName, worker.CurrentJobs)
					busyCount++
				}
			}
		}

		logger.InfoCtx(app.ctx, "‚úÖ Pod deletion priorities set for endpoint %s: %d idle workers (delete first), %d busy workers (delete last)",
			endpoint, idleCount, busyCount)
		logger.InfoCtx(app.ctx, "‚ÑπÔ∏è  Workers will be marked as DRAINING by PodWatcher when K8s actually deletes them (respects maxUnavailable)")
	})

	if err != nil {
		return fmt.Errorf("failed to register deployment watcher: %w", err)
	}

	// Register deployment status change callback to sync status to database
	err = k8sProvider.WatchDeploymentStatusChange(app.ctx, func(endpoint string, deployment *appsv1.Deployment) {
		// Calculate status
		status := "Pending"
		if deployment.Status.AvailableReplicas == *deployment.Spec.Replicas && *deployment.Spec.Replicas > 0 {
			status = "Running"
		} else if *deployment.Spec.Replicas == 0 {
			status = "Stopped"
		}

		logger.InfoCtx(app.ctx, "üìä Deployment status changed for %s: status=%s, replicas=%d/%d/%d",
			endpoint, status, deployment.Status.ReadyReplicas, deployment.Status.AvailableReplicas, *deployment.Spec.Replicas)

		// Update endpoint in database
		if app.mysqlRepo != nil && app.mysqlRepo.Endpoint != nil {
			runtimeState := map[string]interface{}{
				"namespace":         deployment.Namespace,
				"replicas":          *deployment.Spec.Replicas,
				"readyReplicas":     deployment.Status.ReadyReplicas,
				"availableReplicas": deployment.Status.AvailableReplicas,
			}
			// Extract shmSize and volumeMounts
			if info := k8s.DeploymentToAppInfo(deployment); info != nil {
				if info.ShmSize != "" {
					runtimeState["shmSize"] = info.ShmSize
				}
				if len(info.VolumeMounts) > 0 {
					runtimeState["volumeMounts"] = info.VolumeMounts
				}
			}
			if err := app.mysqlRepo.Endpoint.UpdateRuntimeState(app.ctx, endpoint, status, runtimeState); err != nil {
				logger.ErrorCtx(app.ctx, "Failed to update endpoint runtime state: %v", err)
			}
		}
	})

	if err != nil {
		logger.WarnCtx(app.ctx, "Failed to register deployment status watcher: %v", err)
	}

	logger.InfoCtx(app.ctx, "‚úÖ Deployment watcher registered successfully")
	return nil
}

// initHandlers initializes handler layer
func (app *Application) initHandlers() error {
	// Initialize handlers
	app.taskHandler = handler.NewTaskHandler(app.taskService, app.workerService)
	app.workerHandler = handler.NewWorkerHandler(app.workerService, app.taskService, app.deploymentProvider)
	app.statisticsHandler = handler.NewStatisticsHandler(app.statisticsService, app.workerService)
	app.monitoringHandler = handler.NewMonitoringHandler(app.monitoringService)

	// Initialize Endpoint Handler (for K8s or Novita)
	if app.config.K8s.Enabled || app.config.Novita.Enabled {
		if app.deploymentProvider == nil {
			logger.ErrorCtx(app.ctx, "Deployment provider is enabled but provider is nil")
		} else {
			app.endpointHandler = handler.NewEndpointHandler(app.deploymentProvider, app.endpointService, app.workerService)
			if app.config.K8s.Enabled {
				logger.InfoCtx(app.ctx, "Endpoint handler initialized for K8s")
			}
			if app.config.Novita.Enabled {
				logger.InfoCtx(app.ctx, "Endpoint handler initialized for Novita")
			}
		}
	}

	// Initialize Spec Handler
	app.specHandler = handler.NewSpecHandler(app.specService)
	if app.capacityMgr != nil && app.mysqlRepo != nil {
		app.specHandler.SetCapacityManager(app.capacityMgr, app.mysqlRepo.SpecCapacity)
	}

	// Initialize Image Handler (for DockerHub webhook and image update checking)
	if app.endpointService != nil {
		app.imageHandler = handler.NewImageHandler(app.endpointService, &app.config.Docker)
		logger.InfoCtx(app.ctx, "Image handler initialized")
	}

	return nil
}

// initAutoScaler initializes auto-scaler
func (app *Application) initAutoScaler() error {
	if !app.config.K8s.Enabled {
		logger.InfoCtx(app.ctx, "K8s not enabled, skipping autoscaler initialization")
		return nil
	}

	if !app.config.AutoScaler.Enabled {
		logger.InfoCtx(app.ctx, "AutoScaler not enabled")
		return nil
	}

	// Get spec manager from K8s deployment provider
	var specManager *k8s.SpecManager
	if k8sProvider, ok := app.deploymentProvider.(*k8s.K8sDeploymentProvider); ok {
		specManager = k8sProvider.GetSpecManager()
		// Inject spec service into spec manager for database access
		// SpecService implements SpecRepositoryInterface
		if app.specService != nil {
			specManager.SetSpecRepository(app.specService)
			logger.InfoCtx(app.ctx, "Spec service injected into SpecManager - specs will be read from database first")
		}
	} else {
		logger.WarnCtx(app.ctx, "AutoScaler requires K8s deployment provider, skipping initialization")
		return nil
	}

	autoscalerConfig := &autoscaler.Config{
		Enabled:        app.config.AutoScaler.Enabled,
		Interval:       app.config.AutoScaler.Interval,
		MaxGPUCount:    app.config.AutoScaler.MaxGPUCount,
		MaxCPUCores:    app.config.AutoScaler.MaxCPUCores,
		MaxMemoryGB:    app.config.AutoScaler.MaxMemoryGB,
		StarvationTime: app.config.AutoScaler.StarvationTime,
	}

	app.autoscalerMgr = autoscaler.NewManager(
		autoscalerConfig,
		app.deploymentProvider,
		app.endpointService,
		app.workerService,
		app.mysqlRepo.Task,
		app.mysqlRepo.ScalingEvent,
		app.redisClient.GetClient(),
		specManager,
	)

	app.autoscalerHandler = handler.NewAutoScalerHandler(app.autoscalerMgr, app.endpointService)

	return nil
}

// setupPodStatusWatcher syncs Pod runtime state to worker table
func (app *Application) setupPodStatusWatcher(k8sProvider *k8s.K8sDeploymentProvider) error {
	if k8sProvider == nil {
		return nil
	}

	logger.InfoCtx(app.ctx, "Setting up pod status watcher for worker runtime sync...")

	err := k8sProvider.WatchPodStatusChange(app.ctx, func(podName, endpoint string, info *interfaces.PodInfo) {
		var createdAt, startedAt *time.Time
		if info.CreatedAt != "" {
			if t, err := time.Parse(time.RFC3339, info.CreatedAt); err == nil {
				createdAt = &t
			}
		}
		if info.StartedAt != "" {
			if t, err := time.Parse(time.RFC3339, info.StartedAt); err == nil {
				startedAt = &t
			}
		}

		// Check if this is a new worker (for WORKER_STARTED event)
		existingWorker, _ := app.mysqlRepo.Worker.GetByPodName(app.ctx, endpoint, podName)
		isNewWorker := existingWorker == nil

		// Create or update worker (status STARTING until heartbeat)
		if err := app.mysqlRepo.Worker.UpsertFromPod(app.ctx, podName, endpoint, info.Phase, info.Status, info.Reason, info.Message, info.IP, info.NodeName, createdAt, startedAt); err != nil {
			logger.WarnCtx(app.ctx, "Failed to upsert worker from pod %s: %v", podName, err)
		}

		// Record WORKER_STARTED event for new workers
		if isNewWorker && app.workerEventService != nil {
			app.workerEventService.RecordWorkerStarted(app.ctx, podName, endpoint)
		}
	})

	if err != nil {
		return fmt.Errorf("failed to setup pod status watcher: %w", err)
	}

	// Watch pod deletions to mark workers as OFFLINE
	err = k8sProvider.WatchPodDelete(app.ctx, func(podName, endpoint string) {
		// Record WORKER_OFFLINE event before marking offline
		if app.workerEventService != nil {
			app.workerEventService.RecordWorkerOffline(app.ctx, podName, endpoint, podName)
		}
		if err := app.mysqlRepo.Worker.MarkOfflineByPodName(app.ctx, podName); err != nil {
			logger.WarnCtx(app.ctx, "Failed to mark worker offline for deleted pod %s: %v", podName, err)
		}
	})
	if err != nil {
		logger.WarnCtx(app.ctx, "Failed to setup pod delete watcher: %v", err)
	}

	logger.InfoCtx(app.ctx, "‚úÖ Pod status watcher registered successfully")
	return nil
}

// initHTTPServer initializes HTTP server
func (app *Application) initHTTPServer() error {
	// Initialize router
	r := router.NewRouter(app.taskHandler, app.workerHandler, app.endpointHandler, app.autoscalerHandler, app.statisticsHandler, app.specHandler, app.imageHandler, app.monitoringHandler)

	// Set Gin mode
	gin.SetMode(app.config.Server.Mode)

	// Create Gin engine
	app.ginEngine = gin.New()
	app.ginEngine.Use(gin.Recovery())

	// Setup routes
	r.Setup(app.ginEngine)

	// Create HTTP server
	app.httpServer = &http.Server{
		Addr:    fmt.Sprintf(":%d", app.config.Server.Port),
		Handler: app.ginEngine,
	}

	return nil
}

// setupCapacityManager sets up capacity manager for spec availability tracking
func (app *Application) setupCapacityManager(k8sProvider *k8s.K8sDeploymentProvider) error {
	if k8sProvider == nil {
		logger.InfoCtx(app.ctx, "K8s provider not available, skipping capacity manager setup")
		return nil
	}

	logger.InfoCtx(app.ctx, "Setting up capacity manager for platform: %s", app.config.K8s.Platform)

	// Ensure spec manager has database access
	specMgr := k8sProvider.GetSpecManager()
	if specMgr != nil && app.specService != nil {
		specMgr.SetSpecRepository(app.specService)
	}

	// Select provider based on platform
	var provider capacity.Provider
	var spotChecker *capacity.AWSSpotChecker

	switch app.config.K8s.Platform {
	case "aws-eks":
		// AWS EKS with Karpenter - use NodeClaim watch
		dynamicClient := k8sProvider.GetDynamicClient()
		if dynamicClient != nil && specMgr != nil {
			nodePoolToSpec := specMgr.GetNodePoolToSpecMapping("aws-eks")
			if len(nodePoolToSpec) > 0 {
				logger.InfoCtx(app.ctx, "Using Karpenter provider with %d nodepool mappings: %v", len(nodePoolToSpec), nodePoolToSpec)
				provider = capacity.NewProvider(capacity.ProviderKarpenter, dynamicClient, nodePoolToSpec)
			} else {
				logger.WarnCtx(app.ctx, "No nodepool mappings found, falling back to generic provider")
				provider = capacity.NewProvider(capacity.ProviderGeneric, nil, nil)
			}

			// Setup AWS Spot Checker
			specToInstance := specMgr.GetSpecToInstanceTypeMapping("aws-eks")
			specToNodePool := specMgr.GetSpecToNodePoolMapping("aws-eks")
			if len(specToInstance) > 0 || len(specToNodePool) > 0 {
				ec2Client, region, err := createEC2Client(app.ctx, app.config.K8s.AWS)
				if err != nil {
					logger.WarnCtx(app.ctx, "Failed to create EC2 client for spot checker: %v", err)
				} else {
					spotChecker = capacity.NewAWSSpotChecker(ec2Client, region, specToInstance)
					// ËÆæÁΩÆ NodePool fetcher Áî®‰∫éËé∑ÂèñÊ≤°ÊúâÈÖçÁΩÆ instance-type ÁöÑ spec
					spotChecker.SetNodePoolFetcher(k8sProvider, specToNodePool)
					logger.InfoCtx(app.ctx, "AWS Spot checker enabled: %d from config, %d from nodepool", len(specToInstance), len(specToNodePool))
				}
			}
		} else {
			logger.WarnCtx(app.ctx, "Dynamic client or spec manager not available, falling back to generic provider")
			provider = capacity.NewProvider(capacity.ProviderGeneric, nil, nil)
		}
	default:
		// aliyun-ack, generic, etc - use generic provider
		provider = capacity.NewProvider(capacity.ProviderGeneric, nil, nil)
	}

	// Create capacity manager
	app.capacityMgr = capacity.NewManager(provider, app.mysqlRepo.SpecCapacity)

	// Set pod count provider for running/pending count updates
	app.capacityMgr.SetPodCountProvider(&k8sPodCountAdapter{provider: k8sProvider})

	// Set spot checker if available
	if spotChecker != nil {
		app.capacityMgr.SetSpotChecker(spotChecker)
	}

	// Set capacity manager on spec manager
	if specMgr != nil {
		specMgr.SetCapacityManager(app.capacityMgr)
	}

	// Start capacity manager in background
	go func() {
		if err := app.capacityMgr.Start(app.ctx); err != nil {
			logger.WarnCtx(app.ctx, "Capacity manager stopped: %v", err)
		}
	}()

	logger.InfoCtx(app.ctx, "‚úÖ Capacity manager setup completed")
	return nil
}

// createEC2Client ÂàõÂª∫ AWS EC2 ÂÆ¢Êà∑Á´Ø
func createEC2Client(ctx context.Context, awsCfg *config.AWSConfig) (*ec2.Client, string, error) {
	var opts []func(*awsconfig.LoadOptions) error

	// Â¶ÇÊûúÈÖçÁΩÆ‰∫Ü region
	if awsCfg != nil && awsCfg.Region != "" {
		opts = append(opts, awsconfig.WithRegion(awsCfg.Region))
	}

	// Â¶ÇÊûúÈÖçÁΩÆ‰∫Ü AK/SK
	if awsCfg != nil && awsCfg.AccessKeyID != "" && awsCfg.SecretAccessKey != "" {
		opts = append(opts, awsconfig.WithCredentialsProvider(
			credentials.NewStaticCredentialsProvider(awsCfg.AccessKeyID, awsCfg.SecretAccessKey, ""),
		))
	}

	cfg, err := awsconfig.LoadDefaultConfig(ctx, opts...)
	if err != nil {
		return nil, "", err
	}

	return ec2.NewFromConfig(cfg), cfg.Region, nil
}

// k8sPodCountAdapter ÈÄÇÈÖç k8s provider Âà∞ capacity.PodCountProvider
type k8sPodCountAdapter struct {
	provider *k8s.K8sDeploymentProvider
}

func (a *k8sPodCountAdapter) GetPodCountsBySpec(ctx context.Context) (map[string]capacity.PodCounts, error) {
	k8sCounts, err := a.provider.GetPodCountsBySpec(ctx)
	if err != nil {
		return nil, err
	}

	result := make(map[string]capacity.PodCounts)
	for specName, counts := range k8sCounts {
		result[specName] = capacity.PodCounts{
			Running: counts.Running,
			Pending: counts.Pending,
		}
	}
	return result, nil
}
